\documentclass[10pt,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx} % Required for inserting images
\usepackage[margin=1in]{geometry} % For margin control
\usepackage{amsmath} % For mathematical equations
\usepackage{hyperref} % For hyperlinks
\usepackage{booktabs} % For professional tables
\usepackage{microtype} % For better typography
\usepackage{natbib} % For citations
\usepackage{listings} % For code listings
\usepackage{xcolor} % For colors in listings

% Configure listings for prompts
\lstset{
    breaklines=true,
    breakatwhitespace=true,
    basicstyle=\ttfamily\footnotesize,
    columns=flexible,
    frame=single,
    rulecolor=\color{black!30},
    backgroundcolor=\color{black!5},
    basewidth={0.5em,0.5em},
    linewidth={\dimexpr\columnwidth-2\tabcolsep\relax},
    xleftmargin=\tabcolsep,
    xrightmargin=\tabcolsep,
    breakindent=0pt,
    aboveskip=\smallskipamount,
    belowskip=\smallskipamount
}

\title{Audio Project}
\author{Alon Bebchuk, 314023516 \\ Ohad Rubin, xxxxxxxxx}
\date{April 12, 2025}

\begin{document}

\maketitle

\begin{abstract}
Recent advances in persuasion modeling have highlighted the importance of multimodal approaches in understanding human persuasive behaviors. While existing work has explored visual and textual modalities, the role of audio in persuasion detection remains understudied. This paper investigates the effectiveness of audio-enhanced models in detecting persuasion strategies within social deduction games. We present a comprehensive comparison of single-task and multi-task approaches using BERT and Whisper models, analyzing their performance across different persuasion strategies. Our experiments on the Werewolf Among Us dataset demonstrate that incorporating audio features alongside textual content can provide valuable signals for persuasion detection. We evaluate our models across six distinct persuasion strategies, providing insights into the challenges and opportunities of audio-enhanced persuasion modeling. Our findings suggest that audio modality can contribute significantly to the accuracy of persuasion strategy detection, particularly in cases where textual content alone may be ambiguous.
\end{abstract}

\section{Introduction}
The ability to detect and understand persuasion strategies is crucial for developing more sophisticated human-computer interaction systems. While existing research has made significant progress in analyzing textual and visual aspects of persuasion, the role of audio features in persuasion detection remains largely unexplored. Audio signals carry rich paralinguistic information—including tone, emphasis, and emotional markers—that could be crucial for accurately identifying persuasive strategies.

Recent work in persuasion modeling has primarily focused on text-based approaches or multimodal systems incorporating visual data. However, the Werewolf Among Us dataset, with its rich collection of game conversations, provides an opportunity to investigate how audio features can enhance persuasion detection. Social deduction games present a particularly challenging and interesting context for this investigation, as players actively employ various persuasion strategies while attempting to influence others' beliefs and decisions.

Our work addresses this gap by developing and evaluating a series of models that leverage both textual and audio modalities for persuasion strategy detection. We explore different architectural approaches, from single-task models focused on individual strategies to multi-task models capable of detecting multiple strategies simultaneously. Our investigation aims to understand how audio features complement textual information in the context of persuasion detection.

The main contributions of our work include:
\begin{itemize}
    \item A comprehensive evaluation of audio-enhanced models for persuasion strategy detection
    \item Novel model architectures that effectively combine text and audio modalities
    \item Empirical analysis of the impact of audio features across different persuasion strategies
\end{itemize}

\section{Related Work}
\subsection{Persuasion Detection in Dialogue}
Previous work in persuasion detection has primarily focused on textual analysis of online forums, social media, and controlled dialogue settings. Research has demonstrated success in identifying persuasion strategies through natural language processing techniques, particularly using transformer-based models. However, these approaches often overlook the rich information contained in speech signals that could provide additional context for strategy detection.

\subsection{Audio-Based Natural Language Processing}
Recent advances in speech processing, particularly through models like Whisper, have shown promising results in various natural language tasks. These developments have opened new possibilities for incorporating audio features in higher-level language understanding tasks. However, the application of such models to persuasion detection remains relatively unexplored.

\subsection{Multi-task Learning in Social Interaction}
Multi-task learning has shown significant promise in social interaction analysis, allowing models to leverage shared representations across related tasks. While previous work has applied multi-task approaches to sentiment analysis and dialogue act classification, their application to persuasion strategy detection, particularly in the context of audio-enhanced models, represents a novel direction.

\section{Method}
Our work investigates the effectiveness of various model architectures in detecting persuasion strategies in single-modal (text only) and  multi-modal (text and audio) game conversations. We explore both single-task and multi-task approaches using BERT and Whisper models, comparing their performance across different strategies.

\subsection{Architecture}

\subsubsection{Single-Task Models}
For BERT-based single-task classification, we fine-tune a pre-trained bert-base-uncased model (FlaxBertForSequenceClassification) with binary classification heads, one for each strategy. Each model receives a structured prompt containing the strategy definition and dialogue context, outputting a binary prediction through a softmax layer.

\noindent For Whisper-based single-task classification, we implement two variants:
\begin{itemize}
    \item V1: Using FlaxWhisperForConditionalGeneration, we can perform binary classification by comparing logits of specific tokens. The model processes both text and audio input, and classification is performed by comparing the logits of "yes" (token ID 6054) and "no" (token ID 4540) tokens in the decoder's final output position.
    
    \item V2: Introduces a novel architecture (FlaxWhisperForSequenceClassification) that combines Whisper's encoder-decoder architecture with a classification head. The model processes both text and audio input through the Whisper encoder, then applies a custom classification pathway consisting of: \begin{itemize}
        \item A projector layer that transforms the encoder's output
        \item A dropout layer for regularization
        \item A final classification layer outputting binary logits
    \end{itemize}
\end{itemize}

\subsubsection{Multi-Task Binary-Label Models}
The multi-task binary-label approach adapts the single-task architectures to handle all strategies simultaneously while maintaining binary classification per strategy:

\begin{itemize}
    \item BERT: Uses the same architecture as single-task but trains on examples from all strategies simultaneously. The strategy is specified in the input prompt, making it a true multi-task learner that must understand both the strategy definition and the dialogue context.
    
    \item Whisper V1: Extends the single-task yes/no token comparison approach to handle multiple strategies, maintaining the same classification mechanism but training on the full strategy set.
    
    \item Whisper V2: Adapts the custom classification head architecture to handle multiple strategies while preserving the binary nature of each strategy prediction.
\end{itemize}

\subsubsection{Multi-Task Multi-Label Models}
The multi-task multi-label approach produces predictions for all strategies simultaneously:

\begin{itemize}
    \item BERT: Modifies the classification head to output multiple binary predictions (one per strategy) using sigmoid activation functions. The model receives only the dialogue context without strategy-specific prompts.
    
    \item Whisper: Adapts the V2 architecture to output multiple binary predictions, processing both audio and text inputs through the encoder before applying the multi-label classification head.
\end{itemize}

\subsection{Data Processing and Input Handling}

\subsubsection{Text Processing}
For both the BERT and Whisper models, we implement a context window of 5 previous utterances. The input prompts are constructed as follows:

\begin{itemize}
    \item Single-task and multi-task binary-label:
    \begin{lstlisting}
Task: Determine whether the last utterance falls under the [strategy] strategy.
Strategy Definition: [strategy] - [definition]
Previous Utterances:
[Context utterances with "Utterance ->" prefix]
Last Utterance:
[Target utterance with "Utterance ->" prefix]
Strategy: [strategy]
Does the last utterance fall under this strategy?
Answer:
    \end{lstlisting}
    
    \item Multi-task multi-label:
    \begin{lstlisting}
Previous Utterances:
[Context utterances with "Utterance ->" prefix]
Last Utterance:
[Target utterance with "Utterance ->" prefix]
    \end{lstlisting}
\end{itemize}

\subsubsection{Audio Processing}
For Whisper models, audio processing involves:
\begin{itemize}
    \item Loading and caching audio data from MP4 files
    \item Converting audio to 16kHz mono channel format
    \item Extracting audio segments corresponding to the context window
    \item Limiting audio context to 30 seconds (clipping from the start if longer)
    \item Applying Whisper's feature extractor to obtain mel spectrograms
\end{itemize}

\subsection{Training and Evaluation}

\subsubsection{Training Process}
\begin{itemize}
    \item Training proceeds for 10 epochs with evaluation on validation data every 2 epochs
    \item Distributed training across available devices using JAX's pmap for data parallelism
    \item Adam optimizer
    \item Linear learning rate schedule with warmup and decay
    \item Gradient normalization using pmean across devices
    \item Loss functions:
    \begin{itemize}
        \item Binary classification: Softmax cross-entropy
        \item Multi-label classification: Sigmoid binary cross-entropy
        \item Whisper V1: Softmax cross-entropy over normalized yes/no token logits \end{itemize}
\end{itemize}

\subsubsection{Evaluation Protocol}
\begin{itemize}
    \item Metrics computed for both validation and test sets: \begin{itemize}
        \item Per-strategy: F1-score, precision, recall, and accuracy
        \item For multi-task models: Both per-strategy and overall metrics
        \item Classification reports including support counts
    \end{itemize}
    \item Model selection based on highest F1-score on validation data among even-numbered epochs
    \item Final evaluation performed on test set
    \item Experiments repeated with 3 different seeds (12, 42, 87) to assess stability
    \item Results reported as mean and standard deviation across seeds
\end{itemize}

\subsection{Hyperparameters}

\subsubsection{BERT Configuration}
\begin{itemize}
    \item Model: bert-base-uncased
    \item Maximum sequence length: 256 tokens
    \item Batch size: 64 (adjusted for multi-device training)
    \item Learning rate: 3e-5
    \item Adam parameters: $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\varepsilon = 10^{-8}$
    \item No warmup steps
\end{itemize}

\subsubsection{Whisper Configuration}
\begin{itemize}
    \item Model: whisper-small
    \item Maximum sequence length: 448 tokens
    \item Batch size: 32-64 (varies by variant, adjusted for multi-device training)
    \item Learning rate: 1e-5
    \item Adam parameters: $\beta_1 = 0.9$, $\beta_2 = 0.99$, $\varepsilon = 10^{-8}$
    \item Warmup steps: 200
    \item Audio sampling rate: 16kHz
    \item Maximum audio context: 30 seconds
\end{itemize}

The Whisper hyperparameters were specifically tuned to address training stability issues, particularly to prevent exploding gradients that were observed with the default configuration. The modified $\beta_2$ value and addition of warmup steps were crucial for stable training.

\subsection{Dataset}
We utilize the YouTube portion of the Werewolf Among Us dataset \cite{bolinlai2023}, which consists of 14.8 hours of gameplay videos spanning 151 game clips. The dataset contains 20,832 utterances in total, with each utterance annotated for the presence of six different persuasion strategies: Identity Declaration, Accusation, Interrogation, Call for Action, Defense, and Evidence.

The dataset is structured as follows:
\begin{itemize}
    \item Split files (train, validation, test) in JSON format containing:
    \begin{itemize}
        \item Game metadata (video\_name, game\_id)
        \item Dialogue entries with utterance transcriptions
        \item Timestamp information for audio alignment
        \item Strategy annotations for each utterance
    \end{itemize}
    \item Corresponding MP4 video files for each game
\end{itemize}

The dataset exhibits significant class imbalance:
\begin{itemize}
    \item 37.9\% of utterances contain no persuasion strategies
    \item The remaining 62.1\% of utterances are unevenly distributed across strategies
    \item Most frequent strategies: Accusation, Interrogation, and Defense
    \item Less frequent strategies: Identity Declaration, Call for Action, and Evidence
\end{itemize}

This imbalanced distribution presents a particular challenge for model training and evaluation, especially for the less frequent strategy categories. Our evaluation metrics and training approach take this imbalance into account to ensure fair assessment of model performance across all strategy categories.

\subsection{Implementation Details}
All models are implemented using JAX and Flax, with the following key components:
\begin{itemize}
    \item Custom TrainState class extending Flax's train\_state with additional logits and loss functions
    \item Efficient data loading using PyTorch's DataLoader with prefetching and caching
    \item Memory-mapped audio file loading for efficient memory usage
    \item Automatic batch size adjustment for multi-device training
    \item Wandb integration for experiment tracking
    \item Comprehensive logging and model checkpointing
\end{itemize}

\end{document}
